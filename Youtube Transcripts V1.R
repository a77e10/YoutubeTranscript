https://rpubs.com/Joaquin_AR/406480

#### Set Up ----
if (!require("pacman")) install.packages("pacman")
pacman::p_load(RSelenium, tune, workflows, dials, hardhat, parsnip, textrecipes, rsample, netstat,netstat,rvest,purrr,tm,stringr,ggplot2,dplyr,tidytext,syuzhet,textdata, tidyr, data.table,WriteXLS,wordcloud,ggwordcloud,gganimate,gifski,png, topicmodels)
dfTranscript <- readRDS("~/YoutubeTranscript/YoutubeTranscript/dfTranscript.Rda") 
dfTranscript <- readRDS("~/YoutubeTranscript/YoutubeTranscript/dfTranscriptFirefox.Rda") 
resHref <- readRDS("~/YoutubeTranscript/YoutubeTranscript/resHref.Rda") 
CheeseVarieties <- read.csv("CheeseVarieties.csv")

#### Scrap Chrome----
# https://stackoverflow.com/questions/51014205/automating-opening-transcript-for-youtube-automatic-generated-captions
rs_driver_object <- rsDriver(browser = 'chrome',
                             chromever = '100.0.4896.60', # binman::list_versions("chromedriver")
                             verbose = FALSE,
                             port = free_port())

# create a client object
remDr <- rs_driver_object$client

#### Channel
remDr$navigate('https://www.youtube.com/c/GavinWebber/videos')

# Scroll hasta abajo
webElem <- remDr$findElement("css", "body")
for (i in c(1:35)) {
  webElem$sendKeysToElement(list(key = "end"))
  Sys.sleep(1)
}

# Look up links
gridLinks <- remDr$findElements(using = 'css selector' ,"#video-title")
# resHeaders <- unlist(lapply(gridLinks, function(x) {x$getElementText()}))
# resHeaders
resHref <- unlist(lapply(gridLinks, function(x) {x$getElementAttribute("href")}))
saveRDS(resHref, file = "resHref.Rda")

#### Función 1x1
get_transcript <- function(url) {
  tryCatch( {
    remDr$navigate(url)
    play_button <- remDr$findElement(using = 'class', value = "ytp-play-button")$clickElement()
    # Sys.sleep(0.5)
    textDate <- unlist(remDr$findElement(using = 'xpath' ,'//*[@id="info-strings"]/yt-formatted-string')$getElementText())
    # Sys.sleep(0.5)
    textLikes <- unlist(remDr$findElement(using = 'xpath' ,'//*[@id="top-level-buttons-computed"]/ytd-toggle-button-renderer[1]/a')$getElementText())
    # Sys.sleep(0.5)
    textTitle <- unlist(remDr$findElement(using = 'xpath' ,"//yt-formatted-string[@class = 'style-scope ytd-video-primary-info-renderer']")$getElementText())
    # Sys.sleep(0.5)
    textViews <- unlist(remDr$findElement(using = 'xpath' ,"//ytd-video-view-count-renderer[@class='style-scope ytd-video-primary-info-renderer']")$getElementText())
    # Sys.sleep(0.5)
    toca3puntos <- remDr$findElement(using = 'xpath', "//button[@aria-label = 'More actions']")$clickElement()
    Sys.sleep(0.5)
    showTranscript <- remDr$findElement(using = 'xpath', "//tp-yt-paper-listbox[@id = 'items']/ytd-menu-service-item-renderer")$clickElement()
    Sys.sleep(0.5)
    Transcript <- remDr$findElement(using = 'xpath' ,"//div[@id = 'segments-container']")
    textTranscript <- Transcript$getElementText() %>% unlist()
    dfTranscript <- data.frame(
      titleCol = textTitle,
      viewsCol = textViews,
      textCol = textTranscript,
      dateCol = textDate,
      likesCol = textLikes,
      urlCol = url,
      stringsAsFactors = FALSE)},
    error = function(e) NULL)
  return(dfTranscript)
}

dfTranscript <- data.frame()
dfTranscript <- bind_rows(dfTranscript,map(resHref[1:5],get_transcript))

saveRDS(dfTranscript, file = "dfTranscript.Rda") #Guardamos el df solamente

#### Scrap con Firefox ----
rD <- rsDriver(browser = "firefox", port = 4567L)
remDr <- remoteDriver()
remDr <- rD[['client']]

get_transcript <- function(url) {
  tryCatch( {
    remDr$navigate(url)
    play_button <- remDr$findElement(using = 'class', value = "ytp-play-button")$clickElement()
    Sys.sleep(2)
    tryCatch( {textDate <- unlist(remDr$findElement(using = 'xpath' ,'//*[@id="info-strings"]/yt-formatted-string')$getElementText())},
              error = function(e) NULL)
    Sys.sleep(2)
    tryCatch( {textLikes <- unlist(remDr$findElement(using = 'xpath' ,'//*[@id="top-level-buttons-computed"]/ytd-toggle-button-renderer[1]/a')$getElementText())},
              error = function(e) NULL)
    Sys.sleep(2)
    textTitle <- unlist(remDr$findElement(using = 'xpath' ,"//yt-formatted-string[@class = 'style-scope ytd-video-primary-info-renderer']")$getElementText())
    Sys.sleep(2)
    textViews <- unlist(remDr$findElement(using = 'xpath' ,"//ytd-video-view-count-renderer[@class='style-scope ytd-video-primary-info-renderer']")$getElementText())
    Sys.sleep(2)
    toca3puntos <- remDr$findElement(using = 'xpath', "//button[@aria-label = 'Más acciones']")$clickElement()
    Sys.sleep(2)
    tryCatch( {showTranscript <- remDr$findElement(using = 'xpath', "//tp-yt-paper-listbox[@id = 'items']/ytd-menu-service-item-renderer")$clickElement()},
              error = function(e) NULL)
    Sys.sleep(2)
    tryCatch( {Transcript <- remDr$findElement(using = 'xpath' ,"//div[@id = 'segments-container']")},
              error = function(e) NULL)
    textTranscript <- Transcript$getElementText() %>% unlist()
    dfTranscript <- data.frame(
      titleCol = textTitle,
      viewsCol = textViews,
      textCol = textTranscript,
      dateCol = textDate,
      likesCol = textLikes,
      urlCol = url,
      stringsAsFactors = FALSE)},
    error = function(e) NULL)
  return(dfTranscript)
}

dfTranscript2 <- data.frame()
resHrefDF <- as.data.frame(resHref)
resHrefMissing <- resHrefDF %>% filter(!resHrefDF$resHref %in% dfTranscript3$urlCol)
dfTranscript2 <- list2DF(map(unlist(resHrefMissing),get_transcript))
dfTranscriptT <- transpose(simplify_all(dfTranscript2))
dfTranscriptT2 <- as.data.frame(dfTranscriptT)
colnames(dfTranscriptT2) <- c('titleCol', 'viewsCol', 'textCol', 'dateCol', 'likesCol', 'urlCol')
dfTranscript <- bind_rows(dfTranscriptT2,dfTranscript3)

saveRDS(dfTranscript, file = "dfTranscriptFirefox.Rda") #Guardamos el df solamente

#### Data Wrangling ----
# cheese list http://www.nourishinteractive.com/healthy-living/free-nutrition-articles/110-list-cheeses
dfTranscriptA <- dfTranscript
dfTranscriptA$textCol <- gsub("[0-9]+", "", dfTranscript$textCol) 
dfTranscriptA$textCol <- gsub(":", "", dfTranscriptA$textCol)
dfTranscriptA$textCol <- gsub("\n", " ", dfTranscriptA$textCol) 
# dfTranscriptA$textCol <- gsub("[Music]", "", dfTranscriptA$textCol) 
dfTranscriptA$viewsCol <- gsub(" vistas", "",dfTranscriptA$viewsCol) # Si se actualiza revisar si figura en español
dfTranscriptA$viewsCol <- gsub(",", "",dfTranscriptA$viewsCol)
dfTranscriptA$viewsCol <- as.numeric(dfTranscriptA$viewsCol)

dfTranscriptB <- dfTranscriptA %>% mutate(group=if_else(str_detect(dfTranscriptA$titleCol,"Ask|(?i)cheeseman"),"Ask the Cheeseman",
                                                        if_else(str_detect(dfTranscriptA$titleCol,"(?i)how|(?i)make|(?i)making|Day"), "Cheesemaking", 
                                                                if_else(str_detect(dfTranscriptA$titleCol,"(?i)test|(?i)taste"), "Testings", "Others"))))
dfTranscriptC <- dfTranscriptB # dejamos el C con sin filtrar y en la clasificación le agregamos la columna de group
dfTranscriptB <- dfTranscriptB %>% filter(!group %in% c('Others')) %>% filter(!is.na(group))

dfTranscriptC %>% ggplot(aes(group)) + geom_bar()
# dfTranscriptC[[1,3]]

# Repetimos la parte de LDA
delete_words <- c('cheese','milk', 'yeah', 'cheeses',"cheese's",'video','lot')
custom_stop_words <- bind_rows(stop_words,
                               tibble(word = stopwords::stopwords("en", source = "stopwords-iso"),
                                      lexicon = "custom"))
unnestWords2 <- dfTranscriptC %>%
  unnest_tokens(word, textCol, token = "words", to_lower = TRUE) %>%
  anti_join(custom_stop_words) %>% filter(!word %in% delete_words) # %>% left_join(countMonth)
unnestWords2$id <-seq.int(nrow(unnestWords2))
WordsCountArticle <- unnestWords2 %>% 
  count(titleCol, word, sort = TRUE) %>%
  ungroup()

# Usamos Unnest y buscamos por palabras solas
countCheese <- dfTranscriptC %>% 
  unnest_tokens(word, textCol, token = "words", to_lower = TRUE) %>% 
  filter(word %in% tolower(CheeseVarieties$Variety)) %>% count(word)

# Buscamos en todo el texto el match, sirve para nombres de más de una palabra.
countCheeseAll <-  dfTranscriptC %>% dplyr::summarise(textCol = paste(textCol, collapse = " ")) #Junto todas las filas de textoCol en una sola celda
countCheeseMatrix <- as.data.frame(str_count(countCheeseAll$textCol[1], pattern = paste(tolower(CheeseVarieties$Variety),""))) %>% rename(SumCheese = "str_count(countCheeseAll$textCol[1], pattern = paste(tolower(CheeseVarieties$Variety), \"\"))") %>%
  mutate(index = seq.int(nrow(CheeseVarieties))) #Hago un count de la celda de countCheeseAll contra el listado de variedades de queso, que lo paso a minúscula y le agrego un espacio en blanco "" para separ quesos de palabras que empiezan así como Butte con butter
CheeseVarieties <- CheeseVarieties %>% mutate(index = seq.int(nrow(CheeseVarieties))) #Como están ordenadas las variedades de quesos las voy a unir en orden con la matriz que generamos (la matriz nos tira los counts por tipo de queso)
countCheeseMatrix <- CheeseVarieties %>% left_join(countCheeseMatrix) %>% 
  filter(SumCheese > 0) %>% arrange(desc(SumCheese))

# Agregamos con la columna con el queso que están preparando o que prima en la charla? 


#### Classification ----
set.seed(1234)
multinews_split <- initial_split(dfTranscriptB, strata = group) # My training set no incluye Others, vamos a enseñarle las 3 clasificaciones básicas que hicimos nosotros de HOW TO / ASK / CHEESEMAKING
multinews_train <- training(multinews_split)
multinews_test <- testing(multinews_split)

multinews_train %>%
  group_by(group) %>% summarise(n = n()) %>%
  mutate(share = n/sum(n)) %>%
  select(group, n, share) %>% arrange(share)

multinews_rec <-
  recipe(group ~ textCol,
         data = multinews_train) %>%
  step_tokenize(textCol) %>%
  step_stopwords(textCol, language = 'en') %>%
  step_tokenfilter(textCol, max_tokens = 1e3, min_times = 100) %>%
  step_tfidf(textCol)

multinews_folds <- vfold_cv(multinews_train)

multi_spec <- multinom_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

multi_spec

#cargamos info de sparce encoding
sparse_bp <- default_recipe_blueprint(composition = "dgCMatrix")
smaller_lambda <- grid_regular(penalty(range = c(-5, 0)), levels = 20)

multi_lasso_wf <- workflow() %>%
  add_recipe(multinews_rec, blueprint = sparse_bp) %>%
  add_model(multi_spec)

multi_lasso_wf

multi_lasso_rs <- tune_grid(
  multi_lasso_wf,
  multinews_folds,
  grid = smaller_lambda,
  control = control_resamples(save_pred = TRUE)
)

multi_lasso_rs

best_acc <- multi_lasso_rs %>%
  show_best("accuracy")

best_acc

choose_acc <- multi_lasso_rs %>%
  select_by_pct_loss(metric = "accuracy", -penalty)

multi_lasso_rs %>%
  collect_predictions() %>%
  filter(penalty == best_acc$penalty) %>%
  filter(id == "Fold01") %>%
  conf_mat(group, .pred_class) %>%
  autoplot(type = "heatmap") +
  scale_y_discrete(labels = function(x) str_wrap(x, 20)) +
  scale_x_discrete(labels = function(x) str_wrap(x, 20))

finalMulti_wf0 <- finalize_workflow(multi_lasso_wf, choose_acc)
#En este momento aplicamos el wf a nuestros datos, por un lado determinamos el modelo con nuestro training set y dps lo evaluamos con el test set. En final fitted están todos los resultados
finalMulti_wf <- last_fit(finalMulti_wf0, multinews_split)
#Nos tiraba un error al tratar de usar el fit del final fitted a través dell pull_wf_fit y dps predict. Usamos como alternativa fit sobre el training set https://stackoverflow.com/questions/63334046/how-to-simulate-last-fit-using-fit-in-tidymodels
finalMulti_fitted <- finalMulti_wf0 %>% fit(training(multinews_split))
dfTranscriptC <- predict(finalMulti_fitted,dfTranscriptC) %>%
  bind_cols(dfTranscriptC)
saveRDS(dfTranscriptC, file = "videoPredMulti.Rda") #Guardamos el df solamente


#LDA ----
delete_words <- c('cheese','milk', 'yeah', 'cheeses',"cheese's",'video','lot', 'uh', 'bit')
custom_stop_words <- bind_rows(stop_words,
                               tibble(word = stopwords::stopwords("en", source = "stopwords-iso"),
                                      lexicon = "custom"))
unnestWords2 <- dfTranscriptA %>%
  unnest_tokens(word, textCol, token = "words", to_lower = TRUE) %>%
  anti_join(custom_stop_words) %>% filter(!word %in% delete_words) # %>% left_join(countMonth)
tt <- unnestWords2 %>% filter(word %in% c('bit', 'uh'))

unnestWords2$id <-seq.int(nrow(unnestWords2))
# unnestWords2 <- unnestWords2 %>% unite('id2', fecha,header,sep = '_', remove = FALSE)
WordsCountArticle <- unnestWords2 %>% 
  count(titleCol, word, sort = TRUE) %>%
  ungroup()
chapters_dtm <- WordsCountArticle %>%
  cast_dtm(titleCol, word, n)
chapters_dtm
chapters_lda <- LDA(chapters_dtm, k = 4, method = "Gibbs", control = list(seed = 1234))
chapters_lda
chapter_topics <- tidy(chapters_lda, matrix = "beta")
top_terms <- chapter_topics %>%
  group_by(topic) %>%
  top_n(20, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

chapters_gamma <- tidy(chapters_lda, matrix = "gamma") %>% rename(titleCol=document)
chapters_gamma <- chapters_gamma %>%
  inner_join(dfTranscriptA)

chapters_gamma %>% filter(str_detect(titleCol, 'Ask the Cheeseman #210'))
gammaCategories <- chapters_gamma %>% arrange(desc(gamma)) %>% 
  distinct(titleCol, .keep_all= TRUE)
View(gammaCategories)
countCategories <- gammaCategories %>% count(topic)
countCategories
# 1 taste / 2 cheeseman Personal,Channel,Fans / 3 cheeseman Technical / 4 How to
# Podríamos hacer una confusion matrix, pero tenemos que tener una clasificación original similar. 
# En la 2 creo que entrarían lo que consideramos en otras cdo asignamos directamente los grupos. Pensar un poco.

#Bigrams #####
unnestBigrams <- dfTranscriptC %>%
  unnest_tokens(bigram, textCol, token = "ngrams", n = 2,to_lower = FALSE) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  unite(bigram, word1, word2, sep = " ")
unnestBigrams$id <-seq.int(nrow(unnestBigrams))

unnestBigrams <- dfTranscriptC %>%
  unnest_tokens(bigram, textCol, 'ngrams', n = 2) %>%    # tokenize bigrams
  mutate(i = row_number()) %>%    # add index for later grouping
  unnest_tokens(word, bigram, drop = FALSE) %>%    # tokenize bigrams into words
  anti_join(custom_stop_words) %>%    # drop rows with stop words
  group_by(i) %>%    # group by bigram index
  filter(n() == 2) %>%    # drop bigram instances where only one word left
  summarise(bigram = unique(bigram)) %>%    # collapse groups to single bigram each
  count(bigram)

unnestBigrams <- unnestBigrams %>% unite('id2', fecha,header,sep = '_', remove = FALSE)
unnestBigrams %>% count(bigram) %>% arrange(desc(n))


##Palabras Clave
cheeses <- paste(c(tolower(CheeseVarieties$Variety)), collapse="|") # para que str_extract lea a las provincias como regex
unnestCheese <- unnestBigrams %>% mutate(chees = str_extract(bigram,cheeses)) %>% 
  drop_na(chees) %>% count(chees) %>% 
  arrange(desc(n))
unnestCheese %>%
  mutate(share = n/sum(n)) %>% top_n(35) %>%
  ggplot(aes(x = reorder(chees, -share), y = share, group = 1)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.2, size = 9))
#podemos incorporar una columna con la info de pbi de cada provincia (podemos incluso comparar si las provincias que reciben más coparticipación y por ende tienen menos producción aparecen menos)

#POS ####
install.packages("cleanNLP")
library(cleanNLP)
cnlp_init_udpipe()
# upos Universal POS tag
annotation <- map_df(unnestWords2$word,cnlp_annotate)
annotationT <- annotation$token

annotationT %>% filter(upos == 'PROPN') %>% count(lemma) %>% arrange(desc(n))
annotationT %>% filter(upos == 'NOUN') %>% count(lemma) %>% arrange(desc(n))
annotationT %>% count(upos) %>% arrange(desc(n))

#AUX ----
?remDr$findElement()
//*[@id="segments-container"]/ytd-transcript-segment-renderer[1]/div/yt-formatted-string
//*[@id="segments-container"]/ytd-transcript-segment-renderer[2]/div/yt-formatted-string
<yt-formatted-string class="segment-text style-scope ytd-transcript-segment-renderer" aria-hidden="true" tabindex="-1">Hi.</yt-formatted-string>
  
  //*[@id="content"]
//*[@id="items"]/ytd-menu-service-item-renderer 
//*[@id="button"]
more_action_btn = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, "//button[@aria-label = 'More actions']")))
more_action_btn.click()


# captions text element
caption_window <- remDr$findElement(using = "class", value = "captions-text")
//*[@id="button"]
yt-formatted-string.ytd-menu-service-item-renderer
ytd-menu-renderer.ytd-video-primary-info-renderer > yt-icon-button:nth-child(3) > button:nth-child(1)
/html/body/ytd-app/ytd-popup-container/tp-yt-iron-dropdown[2]/div/ytd-menu-popup-renderer/tp-yt-paper-listbox/ytd-menu-service-item-renderer/tp-yt-paper-item/yt-formatted-string
# retrieve plain text
text <- caption_window$getElementText()

remDr$navigate("http://www.google.com/ncr")
webElem <- remDr$findElement(using = "css", "[name = 'q']")
webElem$sendKeysToElement(list("R Cran", key = "enter"))

webElems <- remDr$findElements(using = "css selector", "h3")
resHeaders <- unlist(lapply(webElems, function(x) {x$getElementText()}))
resHeaders

remDr$navigate('https://www.youtube.com/c/GavinWebber/videos')

gridLinks <- remDr$findElements(using = 'css selector' ,"#video-title")
resHeaders <- unlist(lapply(gridLinks, function(x) {x$getElementText()}))
# resHeaders <- unlist(lapply(gridLinks, function(x) {x$findElement(using = 'css' ,"#video-title")$getElementText()}))
resHeaders

resHref <- unlist(lapply(gridLinks, function(x) {x$getElementAttribute("href")}))
resHref

remDr$executeScript("window.scrollTo(0,document.body.scrollHeight);")

remDr$navigate('https://www.youtube.com/c/GavinWebber/videos')
webElem <- remDr$findElement("css", "body")
for (i in c(1:15)) {
  webElem$sendKeysToElement(list(key = "end"))
  Sys.sleep(3)
}



last_height = 0 #
repeat {   
  webElem$sendKeysToElement(list(key = "end"))
  Sys.sleep(3) #delay by 3sec to give chance to load. 
  
  # Updated if statement which breaks if we can't scroll further 
  new_height = remDr$executeScript("return document.body.scrollHeight")
  if(unlist(last_height) == unlist(new_height)) {
    break
  } else {
    last_height = new_height
  }
}

str_detect(dfTranscriptA$titleCol,"(?i)taste")
table(str_detect(dfTranscriptA$titleCol,"Ask|(?i)cheeseman"))["TRUE"]
table(str_detect(dfTranscriptA$titleCol,"(?i)how|(?i)make|(?i)making"))["TRUE"]
table(str_detect(dfTranscriptA$titleCol,"Day |Ask"))["TRUE"]
table(str_detect(dfTranscriptA$titleCol,"Ask"))["TRUE"]

table(str_detect(dfTranscriptA$titleCol,"(?i)how | (?i)make | (?i)test | (?i)tast | (?i)ask | (?i)cheeseman"))["FALSE"]

dfTranscriptB <- dfTranscriptA %>% dplyr::filter(str_detect(dfTranscriptA$titleCol,"(?i)how | (?i)making | (?i)make | (?i)test | (?i)taste | (?i)ask | (?i)cheeseman",  negate = TRUE))
countCheeseMatrix %>% filter(SumCheese > 0) %>% arrange(desc(SumCheese))
CheeseVarieties <- CheeseVarieties %>% mutate(index = seq.int(nrow(CheeseVarieties)))
unite(dfTranscriptC,textCol, sep=" ")
filter(word %in% c('mozzarella','brie')) %>% count(word)
filter(word %in% fixed(CheeseVarieties$Variety, ignore_case = TRUE))
str_detect(word, c('mozzarella','brie'))
str_detect(word, fixed(c('mozzarella','brie'), ignore_case = TRUE))

class(countCheese$word)
# subtitle_button <- remDr$findElement(using = "class", value = "ytp-subtitles-button")
# subtitle_button$clickElement()
# textTitle <- unlist(remDr$findElement(using = 'xpath' ,"//yt-formatted-string[@class = 'style-scope ytd-video-primary-info-renderer']")$getElementText())
# textViews <- unlist(remDr$findElement(using = 'xpath' ,'//*[@id="count"]/ytd-video-view-count-renderer/span[1]')$getElementText())
# textDate <- unlist(remDr$findElement(using = 'css' ,".ytd-video-primary-info-renderer")$getElementText())
# textDate <- unlist(remDr$findElement(using = 'css' ,".ytd-video-primary-info-renderer:nth-child(2)")$getElementText())

function(url) {
  tryCatch( {
    remDr$navigate(url)
    play_button <- remDr$findElement(using = 'class', value = "ytp-play-button")$clickElement()
    Sys.sleep(0.5)
    tryCatch( {textDate <- unlist(remDr$findElement(using = 'xpath' ,'//*[@id="info-strings"]/yt-formatted-string')$getElementText())},
              error = function(e) NULL)
    Sys.sleep(0.5)
    tryCatch( {textLikes <- unlist(remDr$findElement(using = 'xpath' ,'//*[@id="top-level-buttons-computed"]/ytd-toggle-button-renderer[1]/a')$getElementText())},
              error = function(e) NULL)
    Sys.sleep(0.5)
    tryCatch( {textTitle <- unlist(remDr$findElement(using = 'xpath' ,"//yt-formatted-string[@class = 'style-scope ytd-video-primary-info-renderer']")$getElementText())},
              error = function(e) NULL)
    Sys.sleep(0.5)
    tryCatch( { textViews <- unlist(remDr$findElement(using = 'xpath' ,"//ytd-video-view-count-renderer[@class='style-scope ytd-video-primary-info-renderer']")$getElementText())},
              error = function(e) NULL)
    Sys.sleep(0.5)
    toca3puntos <- remDr$findElement(using = 'xpath', "//button[@aria-label = 'More actions']")$clickElement()
    Sys.sleep(0.5)
    showTranscript <- remDr$findElement(using = 'xpath', "//tp-yt-paper-listbox[@id = 'items']/ytd-menu-service-item-renderer")$clickElement()
    Sys.sleep(0.5)
    Transcript <- remDr$findElement(using = 'xpath' ,"//div[@id = 'segments-container']")
    textTranscript <- Transcript$getElementText() %>% unlist()
    dfTranscript <- data.frame(
      titleCol = textTitle,
      viewsCol = textViews,
      textCol = textTranscript,
      dateCol = textDate,
      likesCol = textLikes,
      urlCol = url,
      stringsAsFactors = FALSE)},
    error = function(e) NULL)
  return(dfTranscript)
}

function(url) {
  tryCatch( {
    remDr$navigate(url)
    play_button <- remDr$findElement(using = 'class', value = "ytp-play-button")$clickElement()
    Sys.sleep(0.5)
    tryCatch( {textDate <- unlist(remDr$findElement(using = 'xpath' ,'//*[@id="info-strings"]/yt-formatted-string')$getElementText())},
              error = function(e) NULL)
    Sys.sleep(0.5)
    tryCatch( {textLikes <- unlist(remDr$findElement(using = 'xpath' ,'//*[@id="top-level-buttons-computed"]/ytd-toggle-button-renderer[1]/a')$getElementText())},
              error = function(e) NULL)
    Sys.sleep(0.5)
    tryCatch( {textTitle <- unlist(remDr$findElement(using = 'xpath' ,"//yt-formatted-string[@class = 'style-scope ytd-video-primary-info-renderer']")$getElementText())},
              error = function(e) NULL)
    Sys.sleep(0.5)
    tryCatch( { textViews <- unlist(remDr$findElement(using = 'xpath' ,"//ytd-video-view-count-renderer[@class='style-scope ytd-video-primary-info-renderer']")$getElementText())},
              error = function(e) NULL)
    Sys.sleep(0.5)
    toca3puntos <- remDr$findElement(using = 'xpath', "//button[@aria-label = 'More actions']")$clickElement()
    Sys.sleep(0.5)
    showTranscript <- remDr$findElement(using = 'xpath', "//tp-yt-paper-listbox[@id = 'items']/ytd-menu-service-item-renderer")$clickElement()
    Sys.sleep(0.5)
    Transcript <- remDr$findElement(using = 'xpath' ,"//div[@id = 'segments-container']")
    textTranscript <- Transcript$getElementText() %>% unlist()
    dfTranscript <- data.frame(
      titleCol = textTitle,
      viewsCol = textViews,
      textCol = textTranscript,
      dateCol = textDate,
      likesCol = textLikes,
      urlCol = url,
      stringsAsFactors = FALSE)},
    error = function(e) NULL)
  return(dfTranscript)
}
## Scrap con Firefox
rD <- rsDriver(browser = "firefox", port = 4567L)
remDr <- remoteDriver()
remDr <- rD[['client']]

get_transcript <- function(url) {
  tryCatch( {
    remDr$navigate(url)
    play_button <- remDr$findElement(using = 'class', value = "ytp-play-button")$clickElement()
    Sys.sleep(2)
    tryCatch( {textDate <- unlist(remDr$findElement(using = 'xpath' ,'//*[@id="info-strings"]/yt-formatted-string')$getElementText())},
              error = function(e) NULL)
    Sys.sleep(2)
    tryCatch( {textLikes <- unlist(remDr$findElement(using = 'xpath' ,'//*[@id="top-level-buttons-computed"]/ytd-toggle-button-renderer[1]/a')$getElementText())},
              error = function(e) NULL)
    Sys.sleep(2)
    textTitle <- unlist(remDr$findElement(using = 'xpath' ,"//yt-formatted-string[@class = 'style-scope ytd-video-primary-info-renderer']")$getElementText())
    Sys.sleep(2)
    textViews <- unlist(remDr$findElement(using = 'xpath' ,"//ytd-video-view-count-renderer[@class='style-scope ytd-video-primary-info-renderer']")$getElementText())
    Sys.sleep(2)
    toca3puntos <- remDr$findElement(using = 'xpath', "//button[@aria-label = 'Más acciones']")$clickElement()
    Sys.sleep(2)
    tryCatch( {showTranscript <- remDr$findElement(using = 'xpath', "//tp-yt-paper-listbox[@id = 'items']/ytd-menu-service-item-renderer")$clickElement()},
              error = function(e) NULL)
    Sys.sleep(2)
    tryCatch( {Transcript <- remDr$findElement(using = 'xpath' ,"//div[@id = 'segments-container']")},
              error = function(e) NULL)
    textTranscript <- Transcript$getElementText() %>% unlist()
    dfTranscript <- data.frame(
      titleCol = textTitle,
      viewsCol = textViews,
      textCol = textTranscript,
      dateCol = textDate,
      likesCol = textLikes,
      urlCol = url,
      stringsAsFactors = FALSE)},
    error = function(e) NULL)
  return(dfTranscript)
}

dfTranscript2 <- data.frame()
resHrefDF <- as.data.frame(resHref)
resHrefMissing <- resHrefDF %>% filter(!resHrefDF$resHref %in% dfTranscript3$urlCol)
dfTranscript2 <- list2DF(map(unlist(resHrefMissing),get_transcript))
dfTranscriptT <- transpose(simplify_all(dfTranscript2))
dfTranscriptT2 <- as.data.frame(dfTranscriptT)
colnames(dfTranscriptT2) <- c('titleCol', 'viewsCol', 'textCol', 'dateCol', 'likesCol', 'urlCol')
dfTranscript <- bind_rows(dfTranscriptT2,dfTranscript3)

saveRDS(dfTranscript, file = "dfTranscriptFirefox.Rda") #Guardamos el df solamente


# dfTranscript2 <- bind_rows(dfTranscript3,transpose(list2DF(map(unlist(resHrefMissing),get_transcript)))) 
# class(unlist(resHrefMissing))
# xx <- map(unlist(resHrefMissing)[1:2],get_transcript)
# xxdf <- list2DF(xx)
# xxdf <- transpose(xxdf)

# stop autoplay
# play_button <- remDr$findElement(using = 'class', value = "ytp-play-button")
# play_button$clickElement()
# 
# # # activate subtitles
# textDate <- unlist(remDr$findElement(using = 'xpath' ,'//*[@id="info-strings"]/yt-formatted-string')$getElementText())
# textViews <- unlist(remDr$findElement(using = 'xpath' ,"//ytd-video-view-count-renderer[@class='style-scope ytd-video-primary-info-renderer']")$getElementText())
# textTitle <- unlist(remDr$findElement(using = 'xpath' ,"//yt-formatted-string[@class = 'style-scope ytd-video-primary-info-renderer']")$getElementText())
# textLikes <- unlist(remDr$findElement(using = 'xpath' ,'//*[@id="top-level-buttons-computed"]/ytd-toggle-button-renderer[1]/a')$getElementText())
# 
# #tocar 3 puntos
# toca3puntos <- remDr$findElement(using = 'xpath', "//button[@aria-label = 'More actions']")
# toca3puntos$clickElement()
# 
# showTranscript <- remDr$findElement(using = 'xpath', "//tp-yt-paper-listbox[@id = 'items']/ytd-menu-service-item-renderer")
# showTranscript$clickElement()
# 
# Transcript <- remDr$findElement(using = 'xpath' ,"//div[@id = 'segments-container']")
# text <- Transcript$getElementText() %>% unlist()

str_count(countCheeseAll$textCol[1], pattern = paste(tolower(CheeseVarieties$Variety),""))
paste(tolower(CheeseVarieties$Variety),"")

# annotation <- cnlp_annotate(input = c(
#   dfTranscriptC[[1,3]]
# ))
# annotationDF <- annotation$token
# View(annotationDF)
# 
# vectTranscript <- as.vector(dfTranscriptC$titleCol)
# annotation <- map(input = vectTranscript,cnlp_annotate)
# 
# res <- list()
# annotation <- for (i in 1:588) {
#   res[i] <- cnlp_annotate(input = c(
#     dfTranscriptC[[i,3]]
#   ))
# }
# resDF <- do.call(rbind, res)
# 
# unnestWords2 <- unnestWords2 %>% filter(!is.na(word))
# annotation <- map(unnestWords2,cnlp_annotate)



annotation2 <- as.data.frame(lapply(annotation, unlist))
annotationDF <- annotation$token
annotationDF <- as.data.frame(do.call(cbind, annotationDF))
annotationDF <- as.data.frame(do.call(cbind, annotation))


tryCatch( {
  for (i in 1:588) {
  res[i] <- cnlp_annotate(input = c(
    dfTranscriptC[[i,3]] )) } },
error = function(e) NULL)


class(cnlp_annotate)
class((list(dfTranscriptC$titleCol)))

PalabrasClave <- c('CFK', 'Kirchner', 'Néstor', 'Cristina Fernandez', 'Cristina')
PalabrasClaveReg <- paste(PalabrasClave, collapse="|") 
BigramsPalabrasClave <- unnestBigrams %>% 
  mutate(term = str_extract(bigram, PalabrasClaveReg)) %>%
  drop_na(term) %>% distinct(id2, .keep_all = TRUE) %>% count(Year)

ecoPositivas <- c('estabilidad', 'empleo', 'equilibrio', 'seguridad', 'producción') #seguridad debe usarse mucho como problemas de seguridad
ecoPositivasReg <- paste(c('estabilidad', 'empleo', 'equilibrio', 'seguridad', 'producción', 'economía'), collapse="|") 
ecoNegativas <- c('inflación', 'deuda', 'crisis', 'desequilibrio', 'desempleo', 'inseguridad', 'delito', 'quiebra', 'convocatoria', 'huida', 'corrida')
ecoNegativasReg <- paste(c('inflación', 'deuda', 'crisis', 'desequilibrio', 'desempleo', 'inseguridad', 'delito', 'quiebra', 'convocatoria', 'huida', 'corrida'), collapse="|") 
BigramsPosAnio2 <- unnestBigrams %>% 
  mutate(term = str_extract(bigram, ecoPositivasReg)) %>% filter(!str_extract(bigram, ecoNegativasReg) %in% ecoNegativas) %>%
  drop_na(term) %>% count(Year, name = "ConteoSigno") %>% mutate(tipo = "Positivas") %>%
  left_join(countYear) %>% mutate(ratio = ConteoSigno/cuentaYear)


